{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1C1pVEIln0F0HKXe5v-PreU9Cs5Yl5LSj",
      "authorship_tag": "ABX9TyNxbzBAullpKpXmsvFAZs73",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d4d0ebcca7fb417fa21d1d153cd49cd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bcd7db2abdd5489cb4d6249b5449a0e1",
              "IPY_MODEL_fb01cb2da1db4c089cecb7786cdfe42a",
              "IPY_MODEL_3a8687b44129459f93851d49815e4061"
            ],
            "layout": "IPY_MODEL_aa522784671741a28885260b230b72ce"
          }
        },
        "bcd7db2abdd5489cb4d6249b5449a0e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c66a31874e53469eaad79733ff4fb4aa",
            "placeholder": "​",
            "style": "IPY_MODEL_cdd4cfe757a4471bac080f93adb306f7",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "fb01cb2da1db4c089cecb7786cdfe42a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d876ce2be07145e3b63105504506f2ab",
            "max": 54528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d000e58b4ed9455ba224ae04558d6c65",
            "value": 54528
          }
        },
        "3a8687b44129459f93851d49815e4061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1206a1ce68c4747bfa0c794906cbf81",
            "placeholder": "​",
            "style": "IPY_MODEL_8e9ec90376264cdcaf9f6104c543811d",
            "value": " 54.5k/54.5k [00:00&lt;00:00, 5.21MB/s]"
          }
        },
        "aa522784671741a28885260b230b72ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c66a31874e53469eaad79733ff4fb4aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdd4cfe757a4471bac080f93adb306f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d876ce2be07145e3b63105504506f2ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d000e58b4ed9455ba224ae04558d6c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1206a1ce68c4747bfa0c794906cbf81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e9ec90376264cdcaf9f6104c543811d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9ad27bf55384d218b6bf49b26430919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5ab44b7fe624a129ceb555706aacbc8",
              "IPY_MODEL_f113b3eabdd64576bca1054a6f8b960f",
              "IPY_MODEL_03300227dd124add82aecec950023018"
            ],
            "layout": "IPY_MODEL_06287806d502454d97febd28ba3da1ad"
          }
        },
        "b5ab44b7fe624a129ceb555706aacbc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6495327afbfa41ee80b56caa45bb1371",
            "placeholder": "​",
            "style": "IPY_MODEL_facd6464f3694a8a8060f8071130993c",
            "value": "tokenizer.json: 100%"
          }
        },
        "f113b3eabdd64576bca1054a6f8b960f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4819f28ff19743f38681b55a22032cdd",
            "max": 9085657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6272a1c1fb72412187c8086b816aebf5",
            "value": 9085657
          }
        },
        "03300227dd124add82aecec950023018": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39e35d6896674e4ea60db288fc3481c0",
            "placeholder": "​",
            "style": "IPY_MODEL_a16d1d07e2f44ca6ba4b050c833e0403",
            "value": " 9.09M/9.09M [00:01&lt;00:00, 7.33MB/s]"
          }
        },
        "06287806d502454d97febd28ba3da1ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6495327afbfa41ee80b56caa45bb1371": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "facd6464f3694a8a8060f8071130993c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4819f28ff19743f38681b55a22032cdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6272a1c1fb72412187c8086b816aebf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39e35d6896674e4ea60db288fc3481c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a16d1d07e2f44ca6ba4b050c833e0403": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dedd182cc79f44799bd3947d3494bb80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fab6cd99d6244b94b6e25dbda13d6652",
              "IPY_MODEL_8808180cb6354c99a6596c597ab39131",
              "IPY_MODEL_7ced575e592c4e0099fd076e2803747f"
            ],
            "layout": "IPY_MODEL_ed43c92aa1c9414f9e14670715f54453"
          }
        },
        "fab6cd99d6244b94b6e25dbda13d6652": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e692a51028244b52be1bf2d8d16ae447",
            "placeholder": "​",
            "style": "IPY_MODEL_54209df780d74ababfe47386661ffc91",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "8808180cb6354c99a6596c597ab39131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45743c3928e74454b55bc1499e65d8ef",
            "max": 296,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2bb7934bef01404fa879dd4d4f8957dc",
            "value": 296
          }
        },
        "7ced575e592c4e0099fd076e2803747f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8bb7148609f4ca7a69a8fa9b02211ab",
            "placeholder": "​",
            "style": "IPY_MODEL_d9ef3d67ed77417e8bcab8e3f77090fd",
            "value": " 296/296 [00:00&lt;00:00, 31.0kB/s]"
          }
        },
        "ed43c92aa1c9414f9e14670715f54453": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e692a51028244b52be1bf2d8d16ae447": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54209df780d74ababfe47386661ffc91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45743c3928e74454b55bc1499e65d8ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bb7934bef01404fa879dd4d4f8957dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8bb7148609f4ca7a69a8fa9b02211ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9ef3d67ed77417e8bcab8e3f77090fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e19dca4cd3a54f6391d0c24a41b4a151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82a1c382237c438392abf6d145030991",
              "IPY_MODEL_df72a393dfb242619d7262b92236a347",
              "IPY_MODEL_bd97bafac8a84ba29a23089f7a0fe2e8"
            ],
            "layout": "IPY_MODEL_8b5c1c22be75400db5bac76f75f9fd37"
          }
        },
        "82a1c382237c438392abf6d145030991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f15fdd5c7e294ce0863c4ce95322eb1c",
            "placeholder": "​",
            "style": "IPY_MODEL_5eca2f821b2243a49299c692ecd68738",
            "value": "config.json: 100%"
          }
        },
        "df72a393dfb242619d7262b92236a347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f015a0901f3497aa1f147d71993bff3",
            "max": 877,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b7e07ab8a42a4bc4b37efb3eef12b169",
            "value": 877
          }
        },
        "bd97bafac8a84ba29a23089f7a0fe2e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8921ac54114943c09c404b11bda4716f",
            "placeholder": "​",
            "style": "IPY_MODEL_bb4bc892c57a410c927887c72215efba",
            "value": " 877/877 [00:00&lt;00:00, 98.3kB/s]"
          }
        },
        "8b5c1c22be75400db5bac76f75f9fd37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f15fdd5c7e294ce0863c4ce95322eb1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5eca2f821b2243a49299c692ecd68738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f015a0901f3497aa1f147d71993bff3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7e07ab8a42a4bc4b37efb3eef12b169": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8921ac54114943c09c404b11bda4716f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb4bc892c57a410c927887c72215efba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9975de1ffcbb4eb8a89ec7877d1a0450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0dffed74be34f9ea5d0f38522ac1c3a",
              "IPY_MODEL_2a5f3f0f45774684a1d60434b3186bc1",
              "IPY_MODEL_f65f8a65884d41e492e952ee08d58a0e"
            ],
            "layout": "IPY_MODEL_6ae97a81037c4ab6a53e0290ffe143ca"
          }
        },
        "e0dffed74be34f9ea5d0f38522ac1c3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_041743de606f4f85b01f3baaabd56d6d",
            "placeholder": "​",
            "style": "IPY_MODEL_a4135c9c173f4eb5ba63d9a0c23082bc",
            "value": "model.safetensors: 100%"
          }
        },
        "2a5f3f0f45774684a1d60434b3186bc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b4dcd3f3c3a4354a66be5355df58530",
            "max": 2471645608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e5d8784320954d95a82c0b8076e57109",
            "value": 2471645608
          }
        },
        "f65f8a65884d41e492e952ee08d58a0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d219a18bff84eabb223f87cdd51b448",
            "placeholder": "​",
            "style": "IPY_MODEL_acb13e0dc8564d93acb2530ad5814bda",
            "value": " 2.47G/2.47G [00:51&lt;00:00, 26.0MB/s]"
          }
        },
        "6ae97a81037c4ab6a53e0290ffe143ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "041743de606f4f85b01f3baaabd56d6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4135c9c173f4eb5ba63d9a0c23082bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b4dcd3f3c3a4354a66be5355df58530": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5d8784320954d95a82c0b8076e57109": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d219a18bff84eabb223f87cdd51b448": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acb13e0dc8564d93acb2530ad5814bda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5caa56e6e7394eb1b92f6984de591489": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72304329118f44d2a54bb3c72c5a475d",
              "IPY_MODEL_41f13e774cfb4597a3d933b8399cd26b",
              "IPY_MODEL_e4df3012b3de4a789e89348d92896617"
            ],
            "layout": "IPY_MODEL_ee498e60d64e4828b79944750e9f9b61"
          }
        },
        "72304329118f44d2a54bb3c72c5a475d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d492917158649f1800805b3e1935d16",
            "placeholder": "​",
            "style": "IPY_MODEL_90c3e6de4c234b1eb751a4c66efa528d",
            "value": "generation_config.json: 100%"
          }
        },
        "41f13e774cfb4597a3d933b8399cd26b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3fef5da2f574d3a992e33d34c6cb89e",
            "max": 189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf0f5c13c5f946baa7b18aa23bdfbfaa",
            "value": 189
          }
        },
        "e4df3012b3de4a789e89348d92896617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2020eee1b89d4840846c5c414bd3e9ec",
            "placeholder": "​",
            "style": "IPY_MODEL_3d58ca6764a84d5aad6d8d0aac7eece1",
            "value": " 189/189 [00:00&lt;00:00, 16.5kB/s]"
          }
        },
        "ee498e60d64e4828b79944750e9f9b61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d492917158649f1800805b3e1935d16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90c3e6de4c234b1eb751a4c66efa528d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3fef5da2f574d3a992e33d34c6cb89e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf0f5c13c5f946baa7b18aa23bdfbfaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2020eee1b89d4840846c5c414bd3e9ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d58ca6764a84d5aad6d8d0aac7eece1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsayyagari/RouteLLM/blob/main/micro_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AMWY8wSNXC3u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import statistics\n",
        "import time\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initiate Model"
      ],
      "metadata": {
        "id": "3pbvQYjoYQ3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL_ID = \"meta-llama/Llama-3.2-1B\"  # or \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "# If required (gated), set HF_TOKEN in your env:\n",
        "# export HF_TOKEN=...\n",
        "\n",
        "token = os.environ.get(\"HF_TOKEN\", None)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=token, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, token=token, dtype=dtype)\n",
        "model.to(device).eval()\n",
        "\n",
        "eos_id = tokenizer.eos_token_id\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "d4d0ebcca7fb417fa21d1d153cd49cd3",
            "bcd7db2abdd5489cb4d6249b5449a0e1",
            "fb01cb2da1db4c089cecb7786cdfe42a",
            "3a8687b44129459f93851d49815e4061",
            "aa522784671741a28885260b230b72ce",
            "c66a31874e53469eaad79733ff4fb4aa",
            "cdd4cfe757a4471bac080f93adb306f7",
            "d876ce2be07145e3b63105504506f2ab",
            "d000e58b4ed9455ba224ae04558d6c65",
            "f1206a1ce68c4747bfa0c794906cbf81",
            "8e9ec90376264cdcaf9f6104c543811d",
            "c9ad27bf55384d218b6bf49b26430919",
            "b5ab44b7fe624a129ceb555706aacbc8",
            "f113b3eabdd64576bca1054a6f8b960f",
            "03300227dd124add82aecec950023018",
            "06287806d502454d97febd28ba3da1ad",
            "6495327afbfa41ee80b56caa45bb1371",
            "facd6464f3694a8a8060f8071130993c",
            "4819f28ff19743f38681b55a22032cdd",
            "6272a1c1fb72412187c8086b816aebf5",
            "39e35d6896674e4ea60db288fc3481c0",
            "a16d1d07e2f44ca6ba4b050c833e0403",
            "dedd182cc79f44799bd3947d3494bb80",
            "fab6cd99d6244b94b6e25dbda13d6652",
            "8808180cb6354c99a6596c597ab39131",
            "7ced575e592c4e0099fd076e2803747f",
            "ed43c92aa1c9414f9e14670715f54453",
            "e692a51028244b52be1bf2d8d16ae447",
            "54209df780d74ababfe47386661ffc91",
            "45743c3928e74454b55bc1499e65d8ef",
            "2bb7934bef01404fa879dd4d4f8957dc",
            "e8bb7148609f4ca7a69a8fa9b02211ab",
            "d9ef3d67ed77417e8bcab8e3f77090fd",
            "e19dca4cd3a54f6391d0c24a41b4a151",
            "82a1c382237c438392abf6d145030991",
            "df72a393dfb242619d7262b92236a347",
            "bd97bafac8a84ba29a23089f7a0fe2e8",
            "8b5c1c22be75400db5bac76f75f9fd37",
            "f15fdd5c7e294ce0863c4ce95322eb1c",
            "5eca2f821b2243a49299c692ecd68738",
            "4f015a0901f3497aa1f147d71993bff3",
            "b7e07ab8a42a4bc4b37efb3eef12b169",
            "8921ac54114943c09c404b11bda4716f",
            "bb4bc892c57a410c927887c72215efba",
            "9975de1ffcbb4eb8a89ec7877d1a0450",
            "e0dffed74be34f9ea5d0f38522ac1c3a",
            "2a5f3f0f45774684a1d60434b3186bc1",
            "f65f8a65884d41e492e952ee08d58a0e",
            "6ae97a81037c4ab6a53e0290ffe143ca",
            "041743de606f4f85b01f3baaabd56d6d",
            "a4135c9c173f4eb5ba63d9a0c23082bc",
            "6b4dcd3f3c3a4354a66be5355df58530",
            "e5d8784320954d95a82c0b8076e57109",
            "4d219a18bff84eabb223f87cdd51b448",
            "acb13e0dc8564d93acb2530ad5814bda",
            "5caa56e6e7394eb1b92f6984de591489",
            "72304329118f44d2a54bb3c72c5a475d",
            "41f13e774cfb4597a3d933b8399cd26b",
            "e4df3012b3de4a789e89348d92896617",
            "ee498e60d64e4828b79944750e9f9b61",
            "3d492917158649f1800805b3e1935d16",
            "90c3e6de4c234b1eb751a4c66efa528d",
            "a3fef5da2f574d3a992e33d34c6cb89e",
            "cf0f5c13c5f946baa7b18aa23bdfbfaa",
            "2020eee1b89d4840846c5c414bd3e9ec",
            "3d58ca6764a84d5aad6d8d0aac7eece1"
          ]
        },
        "id": "VUyTM2PQX5hm",
        "outputId": "f3768aae-9f05-45e6-8f0d-3c40b8900be4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4d0ebcca7fb417fa21d1d153cd49cd3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9ad27bf55384d218b6bf49b26430919"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dedd182cc79f44799bd3947d3494bb80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e19dca4cd3a54f6391d0c24a41b4a151"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9975de1ffcbb4eb8a89ec7877d1a0450"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5caa56e6e7394eb1b92f6984de591489"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_next_token(logits, temperature=0.8, top_k=50):\n",
        "    # logits: [batch, vocab]\n",
        "    if temperature <= 0:\n",
        "        return torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "    logits = logits / temperature\n",
        "    print(\"logits shape\", logits.shape)\n",
        "    if top_k is not None:\n",
        "        topk_vals, topk_idx = torch.topk(logits, k=min(top_k, logits.size(-1)), dim=-1)\n",
        "        probs = torch.softmax(topk_vals, dim=-1)\n",
        "        next_local = torch.multinomial(probs, num_samples=1)              # [batch, 1]\n",
        "        next_token = topk_idx.gather(-1, next_local)                      # [batch, 1]\n",
        "        return next_token\n",
        "\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    return torch.multinomial(probs, num_samples=1)"
      ],
      "metadata": {
        "id": "zFMxZmVNXpIZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def generate(prompt, max_new_tokens=80, temperature=0.8, top_k=50):\n",
        "    # Encode prompt once\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    # First forward pass over the whole prompt\n",
        "    out = model(input_ids=input_ids, use_cache=True)\n",
        "    # logits shape: [batch, seq_len, vocab_size]\n",
        "    # KV cache:\n",
        "    past = out.past_key_values\n",
        "\n",
        "    generated = []\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        next_logits = out.logits[:, -1, :]                 # [batch, vocab]\n",
        "        next_id = sample_next_token(next_logits, temperature, top_k)  # [batch, 1]\n",
        "\n",
        "        if eos_id is not None and (next_id == eos_id).all():\n",
        "            break\n",
        "\n",
        "        generated.append(next_id)\n",
        "\n",
        "        # Next step: feed ONLY the new token + KV cache\n",
        "        out = model(input_ids=next_id, past_key_values=past, use_cache=True)\n",
        "        past = out.past_key_values\n",
        "\n",
        "    if generated:\n",
        "        gen_ids = torch.cat(generated, dim=1)              # [batch, new_len]\n",
        "        return tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "    return \"\"\n"
      ],
      "metadata": {
        "id": "QXOkQMSpYZwY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(\"Explain KV cache in one short paragraph:\", max_new_tokens=80))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBVirUFjYeZy",
        "outputId": "990050fa-94cd-47b8-86d7-9551b764a58f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            "logits shape torch.Size([1, 128256])\n",
            " Knowledge, Value, Cache.\n",
            "\n",
            "Knowledge is the data you have about a particular topic. Value is the purpose or benefit of that knowledge. Cache is computer memory.\n",
            "\n",
            "In most cases, a cache is used to store frequently accessed data. Here's how it works: \n",
            "\n",
            "1. The computer's central processing unit (CPU) requests data to be retrieved from the main memory.\n",
            "2. The cache is a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply repetition penality\n",
        "\n",
        "### Apply top-p, top-k filtering"
      ],
      "metadata": {
        "id": "s9dIIoLPf1US"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def apply_repetition_penalty_(logits: torch.Tensor, generated_ids: torch.Tensor, penalty: float):\n",
        "    \"\"\"\n",
        "    logits: [batch, vocab]\n",
        "    generated_ids: [batch, total_generated] token ids seen so far (can include prompt if you want)\n",
        "    penalty > 1.0 discourages repeats\n",
        "    \"\"\"\n",
        "    if penalty is None or penalty <= 1.0 or generated_ids.numel() == 0:\n",
        "        return logits\n",
        "\n",
        "    # Apply penalty per token id that has appeared (common simple approach)\n",
        "    # For each token that was generated, modify its logit.\n",
        "    for b in range(logits.size(0)):\n",
        "        seen = torch.unique(generated_ids[b])\n",
        "        # If logit > 0 => divide by penalty; else multiply by penalty (matches common implementations)\n",
        "        l = logits[b, seen]\n",
        "        logits[b, seen] = torch.where(l > 0, l / penalty, l * penalty)\n",
        "    return logits\n",
        "\n",
        "def top_k_top_p_filtering(logits: torch.Tensor, top_k: int = 0, top_p: float = 1.0):\n",
        "    \"\"\"\n",
        "    logits: [batch, vocab]\n",
        "    returns logits with filtered values set to -inf\n",
        "    \"\"\"\n",
        "    top_k = int(top_k or 0)\n",
        "    top_p = float(top_p if top_p is not None else 1.0)\n",
        "\n",
        "    # Top-K\n",
        "    if top_k > 0:\n",
        "        k = min(top_k, logits.size(-1))\n",
        "        kth_vals = torch.topk(logits, k=k, dim=-1).values[:, -1].unsqueeze(-1)\n",
        "        logits = torch.where(logits < kth_vals, torch.full_like(logits, float(\"-inf\")), logits)\n",
        "\n",
        "    # Top-P (nucleus)\n",
        "    if top_p < 1.0:\n",
        "        sorted_logits, sorted_idx = torch.sort(logits, descending=True, dim=-1)\n",
        "        probs = torch.softmax(sorted_logits, dim=-1)\n",
        "        cumprobs = torch.cumsum(probs, dim=-1)\n",
        "\n",
        "        # Mask tokens with cumulative prob above top_p\n",
        "        mask = cumprobs > top_p\n",
        "        # Keep at least 1 token\n",
        "        mask[:, 0] = False\n",
        "\n",
        "        sorted_logits = torch.where(mask, torch.full_like(sorted_logits, float(\"-inf\")), sorted_logits)\n",
        "        # Scatter back\n",
        "        logits = torch.full_like(logits, float(\"-inf\"))\n",
        "        logits.scatter_(dim=-1, index=sorted_idx, src=sorted_logits)\n",
        "\n",
        "    return logits\n",
        "\n",
        "def sample_from_logits(\n",
        "    logits: torch.Tensor,\n",
        "    temperature: float = 1.0,\n",
        "    top_k: int = 0,\n",
        "    top_p: float = 1.0,\n",
        "):\n",
        "    \"\"\"\n",
        "    logits: [batch, vocab]\n",
        "    returns: [batch, 1]\n",
        "    \"\"\"\n",
        "    if temperature is None:\n",
        "        temperature = 1.0\n",
        "\n",
        "    if temperature <= 0:\n",
        "        return torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "    logits = logits / temperature\n",
        "    logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
        "\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # If everything got filtered (can happen with extreme settings), fall back to argmax\n",
        "    if torch.isnan(probs).any() or (probs.sum(dim=-1) == 0).any():\n",
        "        return torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "    return torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_stream(\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 128,\n",
        "    temperature: float = 0.8,\n",
        "    top_k: int = 40,\n",
        "    top_p: float = 0.95,\n",
        "    repetition_penalty: float = 1.1,\n",
        "    stop_on_eos: bool = True,\n",
        "    print_stream: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Streams decoded text to stdout (optional) and returns the full generated suffix.\n",
        "    \"\"\"\n",
        "    # Encode prompt\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = enc.input_ids.to(device)\n",
        "\n",
        "    # First pass on the whole prompt\n",
        "    out = model(input_ids=input_ids, use_cache=True)\n",
        "    past = out.past_key_values\n",
        "\n",
        "    # Track generated token IDs (for repetition penalty)\n",
        "    generated_ids = torch.empty((input_ids.size(0), 0), dtype=torch.long, device=device)\n",
        "\n",
        "    pieces = []\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        next_logits = out.logits[:, -1, :]  # [batch, vocab]\n",
        "\n",
        "        # Apply repetition penalty based on what we've generated so far\n",
        "        next_logits = apply_repetition_penalty_(next_logits, generated_ids, repetition_penalty)\n",
        "\n",
        "        next_id = sample_from_logits(\n",
        "            next_logits,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "        )  # [batch, 1]\n",
        "\n",
        "        if stop_on_eos and eos_id is not None and (next_id == eos_id).all():\n",
        "            break\n",
        "\n",
        "        generated_ids = torch.cat([generated_ids, next_id], dim=1)\n",
        "\n",
        "        # Decode just the new token for streaming\n",
        "        token_text = tokenizer.decode(next_id[0], skip_special_tokens=True)\n",
        "        pieces.append(token_text)\n",
        "        if print_stream:\n",
        "            sys.stdout.write(token_text)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        # Next step uses KV-cache: feed only the new token\n",
        "        out = model(input_ids=next_id, past_key_values=past, use_cache=True)\n",
        "        past = out.past_key_values\n",
        "\n",
        "    if print_stream:\n",
        "        sys.stdout.write(\"\\n\")\n",
        "\n",
        "    return \"\".join(pieces)\n",
        "\n",
        "# --- Example ---\n",
        "prompt = \"Explain top-p (nucleus) sampling vs top-k in 5-7 sentences:\\n\"\n",
        "suffix = generate_stream(\n",
        "    prompt,\n",
        "    max_new_tokens=140,\n",
        "    temperature=0.8,\n",
        "    top_k=40,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.1,\n",
        "    print_stream=True,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwIuA3BiYhxe",
        "outputId": "7868f377-aec8-4b85-b6ed-dc577eb5435e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-p sampling is a type of random sampling where the number of samples selected from each stratum is proportional to its size, but not necessarily at the top k. Top-k sampling, on the other hand, selects samples based on their position in descending order (e.g., 1st, 2nd, etc.). The main difference between the two approaches is that top-p sampling focuses on the proportionality of sample sizes, while top-k sampling prioritizes the ranking of observations by their likelihood or probability. This means that top-p sampling can be more efficient for estimating probabilities or proportions, while top-k sampling may be better suited for hypothesis testing and confidence intervals.\n",
            "Overall, the choice between top-p\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmark"
      ],
      "metadata": {
        "id": "1nNLuYwRgVJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _decode_loop_with_cache(input_ids, max_new_tokens: int):\n",
        "    \"\"\"\n",
        "    KV-cache decode: prefill once, then feed 1 token per step with past_key_values.\n",
        "    Returns: prefill_s, decode_s, tokens_generated\n",
        "    \"\"\"\n",
        "    t0 = time.perf_counter()\n",
        "    out = model(input_ids=input_ids, use_cache=True)\n",
        "    past = out.past_key_values\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    generated = 0\n",
        "    next_id = torch.argmax(out.logits[:, -1, :], dim=-1, keepdim=True)  # greedy for benchmarking\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if eos_id is not None and (next_id == eos_id).all():\n",
        "            break\n",
        "        out = model(input_ids=next_id, past_key_values=past, use_cache=True)\n",
        "        past = out.past_key_values\n",
        "        next_id = torch.argmax(out.logits[:, -1, :], dim=-1, keepdim=True)\n",
        "        generated += 1\n",
        "\n",
        "    t2 = time.perf_counter()\n",
        "    return (t1 - t0), (t2 - t1), generated\n",
        "\n",
        "\n",
        "def _decode_loop_no_cache(full_ids, max_new_tokens: int):\n",
        "    \"\"\"\n",
        "    No KV-cache: each step re-runs the entire sequence so far (slow).\n",
        "    Returns: prefill_s, decode_s, tokens_generated\n",
        "    \"\"\"\n",
        "    # \"prefill\" is just the first full forward\n",
        "    t0 = time.perf_counter()\n",
        "    out = model(input_ids=full_ids, use_cache=False)\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    generated = 0\n",
        "    next_id = torch.argmax(out.logits[:, -1, :], dim=-1, keepdim=True)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if eos_id is not None and (next_id == eos_id).all():\n",
        "            break\n",
        "        full_ids = torch.cat([full_ids, next_id], dim=1)\n",
        "        out = model(input_ids=full_ids, use_cache=False)\n",
        "        next_id = torch.argmax(out.logits[:, -1, :], dim=-1, keepdim=True)\n",
        "        generated += 1\n",
        "\n",
        "    t2 = time.perf_counter()\n",
        "    return (t1 - t0), (t2 - t1), generated\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def benchmark(\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 64,\n",
        "    runs: int = 5,\n",
        "    warmup: int = 1,\n",
        "    use_cache: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Prints timing stats for CPU:\n",
        "      - prefill seconds\n",
        "      - decode seconds\n",
        "      - decode tokens/sec\n",
        "      - total tokens/sec (prompt tokens + generated tokens) / total time\n",
        "    \"\"\"\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = enc.input_ids.to(device)\n",
        "    prompt_tokens = input_ids.size(1)\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(warmup):\n",
        "        if use_cache:\n",
        "            _decode_loop_with_cache(input_ids, max_new_tokens=8)\n",
        "        else:\n",
        "            _decode_loop_no_cache(input_ids.clone(), max_new_tokens=8)\n",
        "\n",
        "    prefill_s_list = []\n",
        "    decode_s_list = []\n",
        "    gen_list = []\n",
        "\n",
        "    for _ in range(runs):\n",
        "        if use_cache:\n",
        "            prefill_s, decode_s, gen = _decode_loop_with_cache(input_ids, max_new_tokens)\n",
        "        else:\n",
        "            prefill_s, decode_s, gen = _decode_loop_no_cache(input_ids.clone(), max_new_tokens)\n",
        "\n",
        "        prefill_s_list.append(prefill_s)\n",
        "        decode_s_list.append(decode_s)\n",
        "        gen_list.append(gen)\n",
        "\n",
        "    # Aggregate\n",
        "    prefill_med = statistics.median(prefill_s_list)\n",
        "    decode_med = statistics.median(decode_s_list)\n",
        "    gen_med = int(statistics.median(gen_list))\n",
        "\n",
        "    decode_tps = (gen_med / decode_med) if decode_med > 0 else float(\"inf\")\n",
        "    total_tokens = prompt_tokens + gen_med\n",
        "    total_time = prefill_med + decode_med\n",
        "    total_tps = (total_tokens / total_time) if total_time > 0 else float(\"inf\")\n",
        "\n",
        "    mode = \"KV-cache ON\" if use_cache else \"KV-cache OFF\"\n",
        "    print(f\"\\n=== Benchmark ({mode}) ===\")\n",
        "    print(f\"Prompt tokens:       {prompt_tokens}\")\n",
        "    print(f\"Generated tokens:    {gen_med} (target {max_new_tokens})\")\n",
        "    print(f\"Prefill time (med):  {prefill_med:.4f} s\")\n",
        "    print(f\"Decode time (med):   {decode_med:.4f} s\")\n",
        "    print(f\"Decode tok/s:        {decode_tps:.2f}\")\n",
        "    print(f\"Total tok/s:         {total_tps:.2f}  (prompt+gen over prefill+decode)\")\n",
        "    print(f\"Runs: {runs} (warmup {warmup})\")\n",
        "\n",
        "\n",
        "# ---- Example usage ----\n",
        "prompt = \"Write a short explanation of KV cache in LLM inference.\\n\"\n",
        "benchmark(prompt, max_new_tokens=64, runs=5, warmup=1, use_cache=True)\n",
        "benchmark(prompt, max_new_tokens=64, runs=3, warmup=1, use_cache=False)  # fewer runs: it's very slow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3vutvEUgXcv",
        "outputId": "e1057fa4-fea9-4703-b1b1-c7c5f3ad34eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Benchmark (KV-cache ON) ===\n",
            "Prompt tokens:       13\n",
            "Generated tokens:    64 (target 64)\n",
            "Prefill time (med):  0.0206 s\n",
            "Decode time (med):   1.3159 s\n",
            "Decode tok/s:        48.64\n",
            "Total tok/s:         57.61  (prompt+gen over prefill+decode)\n",
            "Runs: 5 (warmup 1)\n",
            "\n",
            "=== Benchmark (KV-cache OFF) ===\n",
            "Prompt tokens:       13\n",
            "Generated tokens:    64 (target 64)\n",
            "Prefill time (med):  0.0241 s\n",
            "Decode time (med):   1.7970 s\n",
            "Decode tok/s:        35.61\n",
            "Total tok/s:         42.28  (prompt+gen over prefill+decode)\n",
            "Runs: 3 (warmup 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmark with optional torch.compile"
      ],
      "metadata": {
        "id": "2gS2_-V9nXLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def maybe_compile(m, use_compile: bool, *, mode=\"reduce-overhead\", fullgraph=False):\n",
        "    \"\"\"\n",
        "    mode: \"reduce-overhead\" (often good for decode), or \"max-autotune\" (can be slower to compile)\n",
        "    fullgraph=False is safer with Transformers.\n",
        "    \"\"\"\n",
        "    if not use_compile:\n",
        "        return m\n",
        "    if not hasattr(torch, \"compile\"):\n",
        "        raise RuntimeError(\"torch.compile not available. Install PyTorch 2.x.\")\n",
        "    return torch.compile(m, mode=mode, fullgraph=fullgraph)\n",
        "\n",
        "@torch.inference_mode()\n",
        "def _decode_loop_with_cache(m, input_ids, max_new_tokens: int):\n",
        "    \"\"\"\n",
        "    KV-cache decode: prefill once, then feed 1 token per step with past_key_values.\n",
        "    Returns: prefill_s, decode_s, tokens_generated\n",
        "    \"\"\"\n",
        "    t0 = time.perf_counter()\n",
        "    out = m(input_ids=input_ids, use_cache=True)\n",
        "    past = out.past_key_values\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    generated = 0\n",
        "    next_id = torch.argmax(out.logits[:, -1, :], dim=-1, keepdim=True)  # greedy for benchmarking\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if eos_id is not None and (next_id == eos_id).all():\n",
        "            break\n",
        "        out = m(input_ids=next_id, past_key_values=past, use_cache=True)\n",
        "        past = out.past_key_values\n",
        "        next_id = torch.argmax(out.logits[:, -1, :], dim=-1, keepdim=True)\n",
        "        generated += 1\n",
        "\n",
        "    t2 = time.perf_counter()\n",
        "    return (t1 - t0), (t2 - t1), generated\n",
        "\n",
        "@torch.inference_mode()\n",
        "def _decode_loop_no_cache(m, full_ids, max_new_tokens: int):\n",
        "    \"\"\"\n",
        "    No KV-cache: each step re-runs the entire sequence so far (slow).\n",
        "    Returns: prefill_s, decode_s, tokens_generated\n",
        "    \"\"\"\n",
        "    t0 = time.perf_counter()\n",
        "    out = m(input_ids=full_ids, use_cache=False)\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    generated = 0\n",
        "    next_id = torch.argmax(out.logits[:, -1, :], dim=-1, keepdim=True)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if eos_id is not None and (next_id == eos_id).all():\n",
        "            break\n",
        "        full_ids = torch.cat([full_ids, next_id], dim=1)\n",
        "        out = m(input_ids=full_ids, use_cache=False)\n",
        "        next_id = torch.argmax(out.logits[:, -1, :], dim=-1, keepdim=True)\n",
        "        generated += 1\n",
        "\n",
        "    t2 = time.perf_counter()\n",
        "    return (t1 - t0), (t2 - t1), generated\n",
        "\n",
        "def benchmark(\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 64,\n",
        "    runs: int = 5,\n",
        "    warmup: int = 1,\n",
        "    use_cache: bool = True,\n",
        "    use_compile: bool = False,\n",
        "    compile_mode: str = \"reduce-overhead\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Benchmarks median prefill/decode times.\n",
        "    Warmup runs include compilation overhead so measured runs don't.\n",
        "    \"\"\"\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = enc.input_ids.to(device)\n",
        "    prompt_tokens = input_ids.size(1)\n",
        "\n",
        "    m = maybe_compile(model, use_compile, mode=compile_mode, fullgraph=False)\n",
        "\n",
        "    # Warmup (triggers compilation if enabled)\n",
        "    for _ in range(warmup):\n",
        "        if use_cache:\n",
        "            _decode_loop_with_cache(m, input_ids, max_new_tokens=8)\n",
        "        else:\n",
        "            _decode_loop_no_cache(m, input_ids.clone(), max_new_tokens=8)\n",
        "\n",
        "    prefill_s_list, decode_s_list, gen_list = [], [], []\n",
        "\n",
        "    for _ in range(runs):\n",
        "        if use_cache:\n",
        "            prefill_s, decode_s, gen = _decode_loop_with_cache(m, input_ids, max_new_tokens)\n",
        "        else:\n",
        "            prefill_s, decode_s, gen = _decode_loop_no_cache(m, input_ids.clone(), max_new_tokens)\n",
        "\n",
        "        prefill_s_list.append(prefill_s)\n",
        "        decode_s_list.append(decode_s)\n",
        "        gen_list.append(gen)\n",
        "\n",
        "    prefill_med = statistics.median(prefill_s_list)\n",
        "    decode_med = statistics.median(decode_s_list)\n",
        "    gen_med = int(statistics.median(gen_list))\n",
        "\n",
        "    decode_tps = (gen_med / decode_med) if decode_med > 0 else float(\"inf\")\n",
        "    total_tokens = prompt_tokens + gen_med\n",
        "    total_time = prefill_med + decode_med\n",
        "    total_tps = (total_tokens / total_time) if total_time > 0 else float(\"inf\")\n",
        "\n",
        "    mode = []\n",
        "    mode.append(\"KV-cache ON\" if use_cache else \"KV-cache OFF\")\n",
        "    mode.append(\"compile ON\" if use_compile else \"compile OFF\")\n",
        "    if use_compile:\n",
        "        mode.append(f\"({compile_mode})\")\n",
        "\n",
        "    print(f\"\\n=== Benchmark [{' | '.join(mode)}] ===\")\n",
        "    print(f\"Prompt tokens:       {prompt_tokens}\")\n",
        "    print(f\"Generated tokens:    {gen_med} (target {max_new_tokens})\")\n",
        "    print(f\"Prefill time (med):  {prefill_med:.4f} s\")\n",
        "    print(f\"Decode time (med):   {decode_med:.4f} s\")\n",
        "    print(f\"Decode tok/s:        {decode_tps:.2f}\")\n",
        "    print(f\"Total tok/s:         {total_tps:.2f}  (prompt+gen over prefill+decode)\")\n",
        "    print(f\"Runs: {runs} (warmup {warmup})\")\n",
        "\n",
        "# ---- Example usage ----\n",
        "prompt = \"Write a short explanation of KV cache in LLM inference.\\n\"\n",
        "\n",
        "# Eager baseline\n",
        "benchmark(prompt, max_new_tokens=64, runs=5, warmup=1, use_cache=True,  use_compile=False)\n",
        "benchmark(prompt, max_new_tokens=64, runs=3, warmup=1, use_cache=False, use_compile=False)\n",
        "\n",
        "# torch.compile\n",
        "# benchmark(prompt, max_new_tokens=64, runs=5, warmup=1, use_cache=True,  use_compile=True, compile_mode=\"reduce-overhead\")\n",
        "# benchmark(prompt, max_new_tokens=64, runs=3, warmup=1, use_cache=False, use_compile=True, compile_mode=\"reduce-overhead\")\n",
        "benchmark(prompt, max_new_tokens=64, runs=5, warmup=1, use_cache=True,  use_compile=True, compile_mode=\"default\")\n",
        "benchmark(prompt, max_new_tokens=64, runs=3, warmup=1, use_cache=False, use_compile=True, compile_mode=\"default\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l45d_cJ9ndjc",
        "outputId": "173d3fcf-2a7a-46b1-b97c-ad521417f333"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Benchmark [KV-cache ON | compile OFF] ===\n",
            "Prompt tokens:       13\n",
            "Generated tokens:    64 (target 64)\n",
            "Prefill time (med):  0.0220 s\n",
            "Decode time (med):   1.3164 s\n",
            "Decode tok/s:        48.62\n",
            "Total tok/s:         57.53  (prompt+gen over prefill+decode)\n",
            "Runs: 5 (warmup 1)\n",
            "\n",
            "=== Benchmark [KV-cache OFF | compile OFF] ===\n",
            "Prompt tokens:       13\n",
            "Generated tokens:    64 (target 64)\n",
            "Prefill time (med):  0.0236 s\n",
            "Decode time (med):   2.0557 s\n",
            "Decode tok/s:        31.13\n",
            "Total tok/s:         37.03  (prompt+gen over prefill+decode)\n",
            "Runs: 3 (warmup 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  return torch._C._get_cublas_allow_tf32()\n",
            "W1217 17:18:02.028000 789 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Benchmark [KV-cache ON | compile ON | (default)] ===\n",
            "Prompt tokens:       13\n",
            "Generated tokens:    64 (target 64)\n",
            "Prefill time (med):  0.0070 s\n",
            "Decode time (med):   0.8456 s\n",
            "Decode tok/s:        75.69\n",
            "Total tok/s:         90.31  (prompt+gen over prefill+decode)\n",
            "Runs: 5 (warmup 1)\n",
            "\n",
            "=== Benchmark [KV-cache OFF | compile ON | (default)] ===\n",
            "Prompt tokens:       13\n",
            "Generated tokens:    64 (target 64)\n",
            "Prefill time (med):  0.0063 s\n",
            "Decode time (med):   1.0086 s\n",
            "Decode tok/s:        63.45\n",
            "Total tok/s:         75.87  (prompt+gen over prefill+decode)\n",
            "Runs: 3 (warmup 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmark harness with backend/mode sweep (CPU/GPU/TPU-ready)"
      ],
      "metadata": {
        "id": "BkLhnkG3VUAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ---------- device selection ----------\n",
        "def pick_device():\n",
        "    # TPU (torch_xla) if available + XLA device present\n",
        "    try:\n",
        "        import torch_xla.core.xla_model as xm\n",
        "        return xm.xla_device(), \"xla\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    # CUDA if available\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\"), \"cuda\"\n",
        "    # CPU fallback\n",
        "    return torch.device(\"cpu\"), \"cpu\"\n",
        "\n",
        "device, dev_kind = pick_device()\n",
        "print(\"Device:\", device, \"(kind:\", dev_kind, \")\")\n",
        "\n",
        "# ---------- dtype ----------\n",
        "if dev_kind == \"cuda\":\n",
        "    dtype = torch.float16\n",
        "elif dev_kind == \"xla\":\n",
        "    # BF16 is commonly a good choice on TPU if supported\n",
        "    dtype = torch.bfloat16\n",
        "else:\n",
        "    dtype = torch.float32\n",
        "\n",
        "# ---------- sync helpers ----------\n",
        "def sync():\n",
        "    if dev_kind == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    elif dev_kind == \"xla\":\n",
        "        import torch_xla.core.xla_model as xm\n",
        "        xm.mark_step()\n",
        "\n",
        "# ---------- compilation ----------\n",
        "def maybe_compile(m, use_compile: bool, backend: str, mode: str):\n",
        "    if not use_compile:\n",
        "        return m\n",
        "    if not hasattr(torch, \"compile\"):\n",
        "        raise RuntimeError(\"torch.compile not available (need PyTorch 2.x).\")\n",
        "    # TPU: PyTorch/XLA integrates with torch.compile via backend='openxla'\n",
        "    # See PyTorch/XLA torch.compile docs.\n",
        "    return torch.compile(m, backend=backend, mode=mode, fullgraph=False)\n",
        "\n",
        "# ---------- decode kernels ----------\n",
        "@torch.inference_mode()\n",
        "def run_with_cache(m, input_ids, max_new_tokens: int):\n",
        "    # prefill\n",
        "    sync()\n",
        "    t0 = time.perf_counter()\n",
        "    out = m(input_ids=input_ids, use_cache=True)\n",
        "    sync()\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    past = out.past_key_values\n",
        "    next_id = torch.argmax(out.logits[:, -1, :], dim=-1, keepdim=True)\n",
        "    gen = 0\n",
        "\n",
        "    # decode\n",
        "    sync()\n",
        "    t2 = time.perf_counter()\n",
        "    for _ in range(max_new_tokens):\n",
        "        if eos_id is not None and (next_id == eos_id).all():\n",
        "            break\n",
        "        out = m(input_ids=next_id, past_key_values=past, use_cache=True)\n",
        "        past = out.past_key_values\n",
        "        next_id = torch.argmax(out.logits[:, -1, :], dim=-1, keepdim=True)\n",
        "        gen += 1\n",
        "    sync()\n",
        "    t3 = time.perf_counter()\n",
        "\n",
        "    return (t1 - t0), (t3 - t2), gen\n",
        "\n",
        "@torch.inference_mode()\n",
        "def run_no_cache(m, full_ids, max_new_tokens: int):\n",
        "    # prefill == first full forward\n",
        "    sync()\n",
        "    t0 = time.perf_counter()\n",
        "    out = m(input_ids=full_ids, use_cache=False)\n",
        "    sync()\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    next_id = torch.argmax(out.logits[:, -1, :], dim=-1, keepdim=True)\n",
        "    gen = 0\n",
        "\n",
        "    # decode (recompute full sequence each step)\n",
        "    sync()\n",
        "    t2 = time.perf_counter()\n",
        "    for _ in range(max_new_tokens):\n",
        "        if eos_id is not None and (next_id == eos_id).all():\n",
        "            break\n",
        "        full_ids = torch.cat([full_ids, next_id], dim=1)\n",
        "        out = m(input_ids=full_ids, use_cache=False)\n",
        "        next_id = torch.argmax(out.logits[:, -1, :], dim=-1, keepdim=True)\n",
        "        gen += 1\n",
        "    sync()\n",
        "    t3 = time.perf_counter()\n",
        "\n",
        "    return (t1 - t0), (t3 - t2), gen\n",
        "\n",
        "def safe_list_backends():\n",
        "    # This is an internal API; may vary by version, so we keep it optional.\n",
        "    try:\n",
        "        import torch._dynamo as dynamo\n",
        "        return sorted(list(dynamo.list_backends()))\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def bench_one(prompt: str, max_new_tokens=64, runs=5, warmup=1,\n",
        "              use_compile=False, backend=\"inductor\", mode=\"default\", use_cache=True):\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = enc.input_ids.to(device)\n",
        "    prompt_toks = input_ids.size(1)\n",
        "\n",
        "    m = maybe_compile(model, use_compile=use_compile, backend=backend, mode=mode)\n",
        "\n",
        "    # warmup: include compilation + caches\n",
        "    for _ in range(warmup):\n",
        "        if use_cache:\n",
        "            run_with_cache(m, input_ids, max_new_tokens=8)\n",
        "        else:\n",
        "            run_no_cache(m, input_ids.clone(), max_new_tokens=8)\n",
        "\n",
        "    prefill_s, decode_s, gens = [], [], []\n",
        "    for _ in range(runs):\n",
        "        if use_cache:\n",
        "            p, d, g = run_with_cache(m, input_ids, max_new_tokens=max_new_tokens)\n",
        "        else:\n",
        "            p, d, g = run_no_cache(m, input_ids.clone(), max_new_tokens=max_new_tokens)\n",
        "        prefill_s.append(p); decode_s.append(d); gens.append(g)\n",
        "\n",
        "    p_med = statistics.median(prefill_s)\n",
        "    d_med = statistics.median(decode_s)\n",
        "    g_med = int(statistics.median(gens))\n",
        "\n",
        "    decode_tps = g_med / d_med if d_med > 0 else float(\"inf\")\n",
        "    total_tps = (prompt_toks + g_med) / (p_med + d_med) if (p_med + d_med) > 0 else float(\"inf\")\n",
        "\n",
        "    return {\n",
        "        \"compile\": use_compile,\n",
        "        \"backend\": backend,\n",
        "        \"mode\": mode,\n",
        "        \"kv_cache\": use_cache,\n",
        "        \"prompt_toks\": prompt_toks,\n",
        "        \"gen_toks\": g_med,\n",
        "        \"prefill_s\": p_med,\n",
        "        \"decode_s\": d_med,\n",
        "        \"decode_tok_s\": decode_tps,\n",
        "        \"total_tok_s\": total_tps,\n",
        "    }\n",
        "\n",
        "def print_rows(rows):\n",
        "    # minimal pretty-print without pandas\n",
        "    cols = [\"compile\",\"backend\",\"mode\",\"kv_cache\",\"prompt_toks\",\"gen_toks\",\"prefill_s\",\"decode_s\",\"decode_tok_s\",\"total_tok_s\"]\n",
        "    header = \" | \".join(f\"{c:>11}\" for c in cols)\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "    for r in rows:\n",
        "        print(\" | \".join([\n",
        "            f\"{str(r['compile']):>11}\",\n",
        "            f\"{r['backend'][:11]:>11}\",\n",
        "            f\"{r['mode'][:11]:>11}\",\n",
        "            f\"{str(r['kv_cache']):>11}\",\n",
        "            f\"{r['prompt_toks']:>11}\",\n",
        "            f\"{r['gen_toks']:>11}\",\n",
        "            f\"{r['prefill_s']:>11.4f}\",\n",
        "            f\"{r['decode_s']:>11.4f}\",\n",
        "            f\"{r['decode_tok_s']:>11.4f}\",\n",
        "            f\"{r['total_tok_s']:>11.2f}\",\n",
        "        ]))\n",
        "\n",
        "# ----------- sweep config -----------\n",
        "prompt = \"Explain KV cache vs recomputing attention in 6 sentences.\\n\"\n",
        "\n",
        "# Choose backends to try.\n",
        "# - Debug/ablation backends: eager, aot_eager (good for isolating failures) :contentReference[oaicite:4]{index=4}\n",
        "# - Performance backend: inductor (default) :contentReference[oaicite:5]{index=5}\n",
        "# - TPU: openxla :contentReference[oaicite:6]{index=6}\n",
        "if dev_kind == \"xla\":\n",
        "    backends = [\"openxla\"]\n",
        "else:\n",
        "    backends = [\"eager\", \"aot_eager\", \"inductor\"]\n",
        "\n",
        "modes = [\"default\", \"reduce-overhead\", \"max-autotune\"]  # trade compile time vs runtime :contentReference[oaicite:7]{index=7}\n",
        "\n",
        "rows = []\n",
        "\n",
        "# Eager baseline (no compile)\n",
        "rows.append(bench_one(prompt, use_compile=False, use_cache=True,  runs=5, warmup=1))\n",
        "rows.append(bench_one(prompt, use_compile=False, use_cache=False, runs=2, warmup=1))  # slow\n",
        "\n",
        "print(\"Done Eager mode --------------------\")\n",
        "\n",
        "# Compile sweeps\n",
        "for b in backends:\n",
        "    for md in modes:\n",
        "        print(f\"Backend: {b},  Mode: {md} ------------------------------------------\")\n",
        "        rows.append(bench_one(prompt, use_compile=True, backend=b, mode=md, use_cache=True,  runs=5, warmup=1))\n",
        "\n",
        "        rows.append(bench_one(prompt, use_compile=True, backend=b, mode=md, use_cache=False, runs=2, warmup=1))\n",
        "\n",
        "\n",
        "print(\"\\nAvailable torch.compile backends (if discoverable):\", safe_list_backends())\n",
        "\n",
        "print_rows(rows)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqD0I56qrdhQ",
        "outputId": "4e4ddf6b-a9f0-4b49-9977-55d08222bc35"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda (kind: cuda )\n",
            "Done Eage mode --------------------\n",
            "Backend: eager,  Mode: default ------------------------------------------\n",
            "Backend: eager,  Mode: reduce-overhead ------------------------------------------\n",
            "Backend: eager,  Mode: max-autotune ------------------------------------------\n",
            "Backend: aot_eager,  Mode: default ------------------------------------------\n",
            "Backend: aot_eager,  Mode: reduce-overhead ------------------------------------------\n",
            "Backend: aot_eager,  Mode: max-autotune ------------------------------------------\n",
            "Backend: inductor,  Mode: default ------------------------------------------\n",
            "Backend: inductor,  Mode: reduce-overhead ------------------------------------------\n",
            "Backend: inductor,  Mode: max-autotune ------------------------------------------\n",
            "\n",
            "Available torch.compile backends (if discoverable): ['cudagraphs', 'inductor', 'openxla', 'tvm']\n",
            "    compile |     backend |        mode |    kv_cache | prompt_toks |    gen_toks |   prefill_s |    decode_s | decode_tok_s | total_tok_s\n",
            "------------------------------------------------------------------------------------------------------------------------------------------\n",
            "      False |    inductor |     default |        True |          15 |          64 |      0.0227 |      1.3097 |     48.8648 |       59.29\n",
            "      False |    inductor |     default |       False |          15 |          64 |      0.0300 |      2.1236 |     30.1382 |       36.68\n",
            "       True |       eager |     default |        True |          15 |          64 |      0.0183 |      1.0924 |     58.5887 |       71.13\n",
            "       True |       eager |     default |       False |          15 |          64 |      0.0170 |      1.7534 |     36.5005 |       44.62\n",
            "       True |       eager | reduce-over |        True |          15 |          64 |      0.0249 |      1.4684 |     43.5851 |       52.90\n",
            "       True |       eager | reduce-over |       False |          15 |          64 |      0.0279 |      1.7355 |     36.8779 |       44.80\n",
            "       True |       eager | max-autotun |        True |          15 |          64 |      0.0271 |      1.5146 |     42.2561 |       51.24\n",
            "       True |       eager | max-autotun |       False |          15 |          64 |      0.0262 |      1.7356 |     36.8738 |       44.84\n",
            "       True |   aot_eager |     default |        True |          15 |          64 |      0.0258 |      1.5508 |     41.2691 |       50.11\n",
            "       True |   aot_eager |     default |       False |          15 |          64 |      0.0266 |      1.7436 |     36.7064 |       44.63\n",
            "       True |   aot_eager | reduce-over |        True |          15 |          64 |      0.0245 |      1.3640 |     46.9200 |       56.90\n",
            "       True |   aot_eager | reduce-over |       False |          15 |          64 |      0.0309 |      1.7925 |     35.7047 |       43.33\n",
            "       True |   aot_eager | max-autotun |        True |          15 |          64 |      0.0243 |      1.3921 |     45.9747 |       55.77\n",
            "       True |   aot_eager | max-autotun |       False |          15 |          64 |      0.0328 |      2.2285 |     28.7193 |       34.94\n",
            "       True |    inductor |     default |        True |          15 |          64 |      0.0285 |      0.8631 |     74.1530 |       88.61\n",
            "       True |    inductor |     default |       False |          15 |          64 |      0.0139 |      1.0466 |     61.1494 |       74.49\n",
            "       True |    inductor | reduce-over |        True |          15 |          64 |      0.0237 |      1.3893 |     46.0655 |       55.91\n",
            "       True |    inductor | reduce-over |       False |          15 |          64 |      0.0272 |      1.7796 |     35.9626 |       43.72\n",
            "       True |    inductor | max-autotun |        True |          15 |          64 |      0.0358 |      1.3637 |     46.9327 |       56.45\n",
            "       True |    inductor | max-autotun |       False |          15 |          64 |      0.0263 |      1.7929 |     35.6961 |       43.43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_fMmPIJJXWDr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}